# About this experiment
name: cheat-mab-0
comment: A3C with LSTM

# About the algorithm
algorithm:
  repository: https://github.com/cipollone/recurrl2.git
  version: null
  commit: null
  diff: null
  name: recurrl

  params:
    agent: IMPALA
    config:
      num_workers: 10
      num_envs_per_worker: 1
      num_gpus: 1
      rollout_fragment_length: 20
      train_batch_size: 30
      batch_mode: truncate_episodes
      opt_type: adam
      lr: null
      gamma: 0.98
      framework: tf
      model:
        fcnet_hiddens: [32]
        fcnet_activation: tanh
        use_lstm: true
        max_seq_len: 20
        lstm_cell_size: 32
    run:
      stop:
        episodes_total: 100000
      num_samples: 5
    tune:
      lr: [0.001]

# About the environment
environment:
  name: cheat_mab
  repository: https://github.com/whitemech/nonmarkov-envs.git
  version: 0.2.3
  commit: 6c8b98a
  diff: ''

  params:
    spec: 
      nb_arms: 2
      win_probs: [0.2, 0.2]
      cheat_sequence: [0, 0, 0, 1]
      reward_win: 1.0
    rdp:
      markovian: false
      episode_length: 10

# Periodic evaluation
evaluation:
  episodes: 50
  frequency: 10000


# How to execute
n-runs: 1
run-command: poetry run python -m recurrl train
output-base: outputs
